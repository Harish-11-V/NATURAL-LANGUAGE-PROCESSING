# NATURAL-LANGUAGE-PROCESSING
Academic lab repository exploring Natural Language Processing concepts including text preprocessing, language modeling, embeddings, HMMs, transformers, and real-world NLP applications using Python.

---

<p align="center">
  <img src="https://img.shields.io/badge/NLP-AI23632-blueviolet?style=for-the-badge">
  <img src="https://img.shields.io/badge/Language%20Models-N--Grams%20%7C%20HMM-orange?style=for-the-badge">
  <img src="https://img.shields.io/badge/Transformers-LLMs-success?style=for-the-badge">
</p>

<h1 align="center">ğŸ—£ï¸ Natural Language Processing</h1>

<p align="center">
  <b>Laboratory Repository For The Course AI23632</b><br>
  B.Tech Artificial Intelligence & Machine Learning
</p>

---

## ğŸ§  About This Repository

This repository contains hands-on laboratory experiments for the **Natural Language Processing (NLP)** course. It focuses on enabling machines to understand, analyze, and generate human language using statistical methods, machine learning techniques, and transformer-based architectures.

---

## ğŸ¯ Course Objectives

- Understand fundamental NLP concepts and language structure  
- Explore morphological analysis and language modeling  
- Implement statistical and probabilistic NLP models  
- Study transformers and large language models  

---

## ğŸ“˜ Unit-Wise Syllabus

### ğŸ”¹ UNIT I â€“ Introduction To NLP
- NLP Overview And Applications  
- Ambiguity Of Language  
- Parts Of Speech And Phrase Structure  
- Entropy, Perplexity, Cross Entropy  
- Text Preprocessing And Corpora Analysis  

---

### ğŸ”¹ UNIT II â€“ Morphology And Language Modeling
- Inflectional And Derivational Morphology  
- Finite State Automata And Transducers  
- Bag Of Words  
- N-Gram Language Models  
- Smoothing Techniques  

---

### ğŸ”¹ UNIT III â€“ Vector Semantics And Embeddings
- Vector Space Models  
- TF-IDF And PMI  
- Word2Vec  
- Semantic Properties And Bias In Embeddings  
- Retrieval-Augmented Generation (RAG)  

---

### ğŸ”¹ UNIT IV â€“ Markov Models And POS Tagging
- Hidden Markov Models  
- POS Tagging  
- Viterbi Algorithm  
- Applications Of HMMs  

---

### ğŸ”¹ UNIT V â€“ Transformers And LLMs
- Transformer Architecture  
- Attention Mechanism  
- Pretraining And Evaluation  
- Sampling Techniques  

---

## ğŸ§ª List Of Experiments

- Text Tokenization And Segmentation  
- POS Tagging Using HMM  
- N-Gram Language Modeling  
- TF-IDF And Cosine Similarity  
- Word Embedding Visualization  
- Sentiment Analysis Using BERT  
- Question Answering Systems  

---

## ğŸ›  Tools And Technologies Used

<p align="center">
  <img src="https://img.shields.io/badge/Python-blue?style=flat&logo=python">
  <img src="https://img.shields.io/badge/NLTK-orange?style=flat">
  <img src="https://img.shields.io/badge/spaCy-blue?style=flat">
  <img src="https://img.shields.io/badge/HuggingFace-yellow?style=flat">
</p>

---

## ğŸ§© Folder Structure

```text
NATURAL-LANGUAGE-PROCESSING
â”‚
â”œâ”€â”€ Unit-01_Introduction
â”œâ”€â”€ Unit-02_Language-Modeling
â”œâ”€â”€ Unit-03_Embeddings
â”œâ”€â”€ Unit-04_HMM-POS
â”œâ”€â”€ Unit-05_Transformers
â”‚
â”œâ”€â”€ datasets
â”œâ”€â”€ notebooks
â””â”€â”€ README.md
```

---

## ğŸŒŸ Learning Outcomes

Students will gain practical knowledge in text analytics, language modeling, embeddings, and transformer-based NLP systems.

---

## ğŸ‘¨â€ğŸ’» Author

Harish Kumar V

B.Tech â€“ Artificial Intelligence & Machine Learning

Rajalakshmi Engineering College

---
